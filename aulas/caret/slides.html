<!DOCTYPE html>
<html>
  <head>
    <title>caret</title>
    <meta charset="utf-8">
    <meta name="author" content="@Curso-R" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# caret
## Machine Learning com R
### <span class="citation">@Curso-R</span>
### 24-05-2018

---




# caret

* Pacote do R!
* Criado pelo Max Kuhn (hoje no RStudio)

![:scale 40%](https://avatars1.githubusercontent.com/u/5731043?s=400&amp;v=4)

* Abreviação de _Classification And Regression Training_
* Primeira versão em 2007

---

# Motivação

* Cada pacote do R tem a sua forma de especificar o modelo
* Cada um também tem a sua forma de especificar predições
* É normal ter este tipo de inconsistências uma vez que são feitos por autores distintos

## Exemplo

Como pedir a estimativa da probabilidade em um modelo binário em diversas funções/pacotes:

* `lda` (MASS): `predict(obj)`
* `glm` (stats): `predict(obj, type = "response")`
* `gbm` (gbm): predict(obj, type = "response", n.trees)
* `rpart` (rpart): `predict(obj, type = "prob")`

---
class: center, middle

# caret

## Menor atrito cognitivo para ajustar muitos modelos

![:scale 70%](https://upload.wikimedia.org/wikipedia/commons/b/b5/A_Ariel_view_of_Carot.jpg)

---

# caret

Inclui funções para:

* separação dos dados
* pré-processamento
* seleção de variáveis
* tuning de modelos e validação cruzada
* estimação de importância de variáveis

Todas criadas de forma padronizada.

![:scale 35%](https://cdn.notonthehighstreet.com/system/product_images/images/002/128/349/original_rabbit-and-carrots-wrapping-paper-set.jpg)

---

# Alternativas

* `mlr`: [https://github.com/mlr-org/mlr](https://github.com/mlr-org/mlr)
* `parsnip`: [https://github.com/topepo/parsnip](https://github.com/topepo/parsnip) (em desenvolvimento)

![:scale 70%](https://www.simplyrecipes.com/wp-content/uploads/2009/12/parsnips-horiz-1800.jpg)

* `scikit-learn`: [https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn) (python)

---

# Exemplo: Diamonds


```r
library(ggplot2)
library(dplyr)
```

```
## 
## Attaching package: 'dplyr'
```

```
## The following objects are masked from 'package:stats':
## 
##     filter, lag
```

```
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
```

```r
diamonds
```

```
## # A tibble: 53,940 x 10
##    carat cut       color clarity depth table price     x     y     z
##    &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 0.230 Ideal     E     SI2      61.5   55.   326  3.95  3.98  2.43
##  2 0.210 Premium   E     SI1      59.8   61.   326  3.89  3.84  2.31
##  3 0.230 Good      E     VS1      56.9   65.   327  4.05  4.07  2.31
##  4 0.290 Premium   I     VS2      62.4   58.   334  4.20  4.23  2.63
##  5 0.310 Good      J     SI2      63.3   58.   335  4.34  4.35  2.75
##  6 0.240 Very Good J     VVS2     62.8   57.   336  3.94  3.96  2.48
##  7 0.240 Very Good I     VVS1     62.3   57.   336  3.95  3.98  2.47
##  8 0.260 Very Good H     SI1      61.9   55.   337  4.07  4.11  2.53
##  9 0.220 Fair      E     VS2      65.1   61.   337  3.87  3.78  2.49
## 10 0.230 Very Good H     VS1      59.4   61.   338  4.00  4.05  2.39
## # ... with 53,930 more rows
```

```r
glimpse(diamonds)
```

```
## Observations: 53,940
## Variables: 10
## $ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ...
## $ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very G...
## $ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, ...
## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI...
## $ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ...
## $ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54...
## $ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,...
## $ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ...
## $ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ...
## $ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ...
```

**Objetivo**: Prever o preço dadas outras características do diamante.

---

# Visualização


```r
library(GGally)
library(dplyr)
colunas &lt;- names(diamonds)
ggduo(
  diamonds %&gt;% sample_n(1000), 
  columnsY = "price", columnsX = colunas[colunas != "price"]
)
```

![](slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---

### Regressão linear



```r
library(caret)
model &lt;- train(price ~ carat + x, data = diamonds, method = "lm")
model
```

```
## Linear Regression 
## 
## 53940 samples
##     2 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 53940, 53940, 53940, 53940, 53940, 53940, ... 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   1531.35  0.8526449  913.7545
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
```
---

### Faznedo predições


```r
preds &lt;- predict(model, diamonds)
head(preds)
```

```
##          1          2          3          4          5          6 
##   10.84233 -130.06601  -91.84336  361.68738  420.44717  122.37077
```

---


### Cross Validation


```r
train_control &lt;- trainControl(method="cv", number=5)
model &lt;- train(
  price ~ carat + x, data = diamonds, 
  method = "lm", 
  trControl = train_control
)
model
```

```
## Linear Regression 
## 
## 53940 samples
##     2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 43152, 43152, 43152, 43152, 43152 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1527.546  0.8534132  914.6353
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
```

---

### Exercício

Como faz para usar LOOCV?



```r
train_control &lt;- trainControl(method="LOOCV")
model &lt;- train(
  price ~ carat + x, data = diamonds, 
  method = "lm", 
  trControl = train_control
)
model
```

---

### Outra função



```r
MSE &lt;- function(data, lev = NULL, model = NULL) {
  out &lt;- sum((data$obs - data$pred)^2)/nrow(data)
  names(out) &lt;- "MSE"
  out
}
```


```r
train_control &lt;- trainControl(method="cv", number=5, summaryFunction = MSE)
model &lt;- train(
  price ~ carat + x, data = diamonds, 
  method = "lm", 
  trControl = train_control,
  metric = "MSE"
)
```

---


```r
model
```

```
## Linear Regression 
## 
## 53940 samples
##     2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 43152, 43152, 43152, 43152, 43152 
## Resampling results:
## 
##   MSE    
##   2334580
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
```


---

### Seleção de variáveis


```r
x &lt;- diamonds %&gt;% select(-price)
x &lt;- model.matrix(~., data = x)
y &lt;- diamonds$price

ctrl &lt;- rfeControl(
  functions = lmFuncs,
  method = "cv",
  number = 5,
  verbose = TRUE
)

model &lt;- rfe(
  x = x, 
  y = y, 
  sizes = c(1, 5, 7), 
  rfeControl = ctrl
)
```

---


```r
model
```

```
## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (5 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables RMSE Rsquared    MAE RMSESD RsquaredSD  MAESD Selected
##          1 1549   0.8494 1007.5  17.12   0.002243  9.485         
##          5 1200   0.9096  820.4  19.49   0.002592 21.420         
##          7 1161   0.9155  763.0  20.29   0.002795  5.559         
##         24 1132   0.9196  740.6  20.56   0.002935  4.328        *
## 
## The top 5 variables (out of 24):
##    carat, clarity.L, color.L, clarity.Q, x
```

---

### Árvores de decisão


```r
train_control &lt;- trainControl(method="cv", number=5)
model &lt;- train(
  price ~ ., data = diamonds, 
  method = "rpart", 
  trControl = train_control
)
```

---


```r
model
```

```
## CART 
## 
## 53940 samples
##     9 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 43152, 43152, 43152, 43152, 43152 
## Resampling results across tuning parameters:
## 
##   cp          RMSE      Rsquared   MAE     
##   0.03365116  1718.449  0.8139764  1149.107
##   0.18605980  2232.829  0.6802030  1565.965
##   0.60834997  3396.287  0.6058028  2513.759
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.03365116.
```



---

### Modificando o grid de parametros


```r
train_control &lt;- trainControl(method="cv", number=5)
tuning_grid &lt;- data.frame(cp = c(0.03365116/2, 0.03365116, 2*0.03365116))
model &lt;- train(
  price ~ ., data = diamonds, 
  method = "rpart", 
  trControl = train_control, 
  tuneGrid = tuning_grid
)
```

---


```r
model
```

```
## CART 
## 
## 53940 samples
##     9 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 43152, 43152, 43152, 43152, 43152 
## Resampling results across tuning parameters:
## 
##   cp          RMSE      Rsquared   MAE      
##   0.01682558  1385.460  0.8793971   888.7445
##   0.03365116  1748.891  0.8075629  1203.9289
##   0.06730232  1809.070  0.7944161  1310.6543
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.01682558.
```

---


```r
plot(model)
```

![](slides_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---

```r
plot(model$finalModel)
text(model$finalModel)
```

![](slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;


---

### Regressão logística


```r
titanic &lt;- readr::read_csv('https://github.com/curso-r/pu.modelos/raw/master/data/titanic-train.csv')

dplyr::glimpse(titanic)
```

```
## Observations: 891
## Variables: 12
## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,...
## $ Survived    &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,...
## $ Pclass      &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,...
## $ Name        &lt;chr&gt; "Braund, Mr. Owen Harris", "Cumings, Mrs. John Bra...
## $ Sex         &lt;chr&gt; "male", "female", "female", "female", "male", "mal...
## $ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ...
## $ SibSp       &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,...
## $ Parch       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,...
## $ Ticket      &lt;chr&gt; "A/5 21171", "PC 17599", "STON/O2. 3101282", "1138...
## $ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, ...
## $ Cabin       &lt;chr&gt; NA, "C85", NA, "C123", NA, NA, "E46", NA, NA, NA, ...
## $ Embarked    &lt;chr&gt; "S", "C", "S", "S", "S", "Q", "S", "S", "S", "C", ...
```

---

### Regressão logística

Exercício: visualizar a relação entre o número de sobreviventes e os preditores: Sex, Age, Fare, Pclass.




---

### Regressão logística

Exercício: ajustar o modelo de regressão logística usando o `caret`.



---

### Árvores de classificação

Exercício: repetir a análise feita com regressão logística utilizando árvores de decisão.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="js/macro.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
